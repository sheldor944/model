{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 02:01:55.021943: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-04-15 02:01:55.042624: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-04-15 02:01:55.052141: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-15 02:01:55.069965: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from keras.models import load_model\n",
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory growth enabled\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1744660918.941260   84139 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1744660919.009698   84139 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1744660919.010200   84139 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    }
   ],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "\n",
    "# GPU Configuration\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if len(physical_devices) > 0:\n",
    "    try:\n",
    "        for device in physical_devices:\n",
    "            tf.config.experimental.set_memory_growth(device, True)\n",
    "        print(\"GPU memory growth enabled\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "# import os\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '-1'  # Force CPU usage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# current working sequence \n",
    "## Assuming model is trained till time x //\n",
    "## So assume X+15 //\n",
    "## Upload it //\n",
    "## wait 15 min //\n",
    "## get new data //\n",
    "## add it to the list // \n",
    "## scale it with load->save \n",
    "## finetune it\n",
    "# repeat "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GLOBAL VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = 0.85\n",
    "sequence_length = 60\n",
    "epochs = 50\n",
    "learning_rate = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init():\n",
    "    stock_data = pd.read_csv(\"runningData15.csv\", delimiter=\",\")\n",
    "    close_prices = stock_data['close'].values.reshape(-1, 1) \n",
    "    scaler = load('scaler.joblib')\n",
    "    test_scaled = scaler.transform(close_prices)\n",
    "    dump(scaler, 'scaler.joblib')\n",
    "\n",
    "    X_test_new = []\n",
    "    y_test_new = []\n",
    "    #Dummy\n",
    "    y_test_new.append(test_scaled[sequence_length -1])\n",
    "    len_test = len(test_scaled)\n",
    "    print(test_scaled.shape)\n",
    "    for i in range(len_test - sequence_length + 1 ):\n",
    "        X_test_new.append(test_scaled[i:i + sequence_length].flatten())  # Flatten the sequence\n",
    "        if i + sequence_length < len_test:\n",
    "            y_test_new.append(test_scaled[i + sequence_length])\n",
    "    X_test_new = np.array(X_test_new)\n",
    "    y_test_new = np.array(y_test_new)\n",
    "    y_test_new_original = scaler.inverse_transform(y_test_new.reshape(-1, 1))\n",
    "    \n",
    "    return X_test_new, y_test_new, y_test_new_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1849, 1)\n",
      "[[3225.115]\n",
      " [3228.115]\n",
      " [3234.57 ]\n",
      " [3230.785]\n",
      " [3235.905]\n",
      " [3231.485]\n",
      " [3231.145]\n",
      " [3229.57 ]\n",
      " [3231.305]\n",
      " [3228.355]\n",
      " [3228.025]\n",
      " [3219.38 ]\n",
      " [3224.71 ]\n",
      " [3222.275]\n",
      " [3227.045]\n",
      " [3230.185]\n",
      " [3224.55 ]\n",
      " [3223.08 ]\n",
      " [3223.955]\n",
      " [3220.545]\n",
      " [3209.53 ]\n",
      " [3208.185]\n",
      " [3215.415]\n",
      " [3223.11 ]\n",
      " [3223.11 ]\n",
      " [3225.105]\n",
      " [3223.44 ]\n",
      " [3226.44 ]\n",
      " [3211.915]\n",
      " [3206.84 ]\n",
      " [3206.85 ]\n",
      " [3196.56 ]\n",
      " [3205.005]\n",
      " [3208.   ]\n",
      " [3204.675]\n",
      " [3202.53 ]\n",
      " [3204.59 ]\n",
      " [3208.605]\n",
      " [3211.25 ]\n",
      " [3209.505]\n",
      " [3205.86 ]\n",
      " [3198.19 ]\n",
      " [3198.045]\n",
      " [3202.63 ]\n",
      " [3204.94 ]\n",
      " [3206.49 ]\n",
      " [3207.65 ]\n",
      " [3210.565]\n",
      " [3210.64 ]\n",
      " [3216.22 ]\n",
      " [3214.135]\n",
      " [3213.375]\n",
      " [3210.345]\n",
      " [3204.495]\n",
      " [3208.155]\n",
      " [3210.04 ]\n",
      " [3211.36 ]\n",
      " [3209.845]\n",
      " [3210.23 ]\n",
      " [3211.225]]\n",
      "====\n",
      "[3211.225]\n"
     ]
    }
   ],
   "source": [
    "scaler = load('scaler.joblib')\n",
    "\n",
    "x, y, y_original = init()\n",
    "x_og = scaler.inverse_transform(x[-1].reshape(-1, 1))\n",
    "print(x_og)\n",
    "print(\"====\")\n",
    "print(y_original[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 Predicting and uploading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from uuid import UUID\n",
    "from typing import Optional\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class PredictionRequest(BaseModel):\n",
    "    stock_id: Optional[UUID] = None\n",
    "    symbol: Optional[str] = None\n",
    "    opening_price: Optional[float] = None\n",
    "    closing_price: Optional[float] = None\n",
    "    high_price: Optional[float] = None\n",
    "    low_price: Optional[float] = None\n",
    "    volume: Optional[float] = None\n",
    "    date: Optional[datetime] = None\n",
    "    prediction_direction: Optional[bool] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def direction(y_original, y_pred):\n",
    "    if y_original < y_pred:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_prediction(y_test_new_original, results):\n",
    "    API_URL = \"http://127.0.0.1:8000/api/v1/prediction\"\n",
    "    HEADERS = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": \"Bearer your_api_key_here\"\n",
    "    }\n",
    "    \n",
    "    # Safely extract the last result value\n",
    "    last_result = results[-1]\n",
    "    if isinstance(last_result, np.ndarray):\n",
    "        last_result = float(last_result.item())  # Properly extract scalar from array\n",
    "    \n",
    "    # Create the prediction request with Pydantic v2 syntax\n",
    "    prediction_request = PredictionRequest(\n",
    "        stock_id='ade11ce0-a353-427d-9ae7-26b948454eab',\n",
    "        symbol=\"XAUUSD\",\n",
    "        opening_price=0.0,\n",
    "        closing_price=last_result,\n",
    "        high_price=0.0,\n",
    "        low_price=0.0,\n",
    "        volume=0.0,\n",
    "        date=datetime.now(),\n",
    "        prediction_direction=direction(y_test_new_original[-1], last_result)\n",
    "    )\n",
    "    \n",
    "    # Convert to dict with proper serialization (Pydantic v2 syntax)\n",
    "    request_data = prediction_request.model_dump()\n",
    "    \n",
    "    # Ensure proper serialization of special types\n",
    "    if request_data['stock_id']:\n",
    "        request_data['stock_id'] = str(request_data['stock_id'])\n",
    "    if request_data['date']:\n",
    "        request_data['date'] = request_data['date'].isoformat()\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(\n",
    "            API_URL,\n",
    "            json=request_data,\n",
    "            headers=HEADERS,\n",
    "            timeout=10\n",
    "        )\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            print(\"Prediction uploaded successfully\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"Failed to upload prediction: {response.status_code} - {response.text}\")\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(f\"Error uploading prediction: {str(e)}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results is like result =[], python list\n",
    "\n",
    "def assume_and_upload(model, X_test, y_test, scaler, y_test_original, results):\n",
    "    current_input = X_test[-1].reshape(1, sequence_length)\n",
    "    prediction = model.predict(current_input, verbose=0)\n",
    "    results.append(scaler.inverse_transform(prediction.reshape(-1, 1)))\n",
    "\n",
    "    print(f\"Predicted: {results[-1]}\")\n",
    "\n",
    "    upload_prediction(y_test_original, results)\n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get New Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLiveData(symbol: str, exchange: str, interval: str, n_bars: int):\n",
    "\n",
    "\n",
    "        from tvDatafeed import TvDatafeedLive, Interval\n",
    "        import pandas as pd\n",
    "\n",
    "\n",
    "        username = 'your_actual_username'\n",
    "        password = 'your_actual_password'\n",
    "\n",
    "        tvl = TvDatafeedLive()\n",
    "\n",
    "        xauusd_data = tvl.get_hist(symbol, exchange, interval=Interval.in_15_minute, n_bars=n_bars, fut_contract=None, extended_session=False, timeout=-1)\n",
    "        # print(xauusd_data)\n",
    "\n",
    "        # seis = tvl.new_seis('ETHUSDT', 'BINANCE', Interval.in_1_hour)\n",
    "        # seis2 = tvl.new_seis('ETHUSDT', 'BINANCE', Interval.in_2_hour)\n",
    "\n",
    "        # ethusdt_data = seis.get_hist(n_bars=10, timeout=-1)\n",
    "        df = pd.DataFrame(xauusd_data)\n",
    "\n",
    "        # Save to CSV\n",
    "        df.to_csv('lastData.csv', index=True)\n",
    "\n",
    "        # print(\"Data saved to lastData.csv\")\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def get_new_data():\n",
    "    while True:\n",
    "        try:\n",
    "            # Get new data\n",
    "            getLiveData('XAUUSD', 'FOREXCOM', '15MIN', 1)\n",
    "            \n",
    "            # Read data - ensure no index is read\n",
    "            main_df = pd.read_csv('runningData15.csv')\n",
    "            new_df = pd.read_csv('lastData.csv')\n",
    "            \n",
    "            # Convert datetime columns\n",
    "            main_df['datetime'] = pd.to_datetime(main_df['datetime'])\n",
    "            new_df['datetime'] = pd.to_datetime(new_df['datetime'])\n",
    "            \n",
    "            # Sort by datetime to ensure chronological order\n",
    "            main_df = main_df.sort_values('datetime')\n",
    "            new_df = new_df.sort_values('datetime')\n",
    "            \n",
    "            # Get timestamps\n",
    "            last_datetime = main_df['datetime'].iloc[-1]\n",
    "            new_datetime = new_df['datetime'].iloc[0]\n",
    "            \n",
    "            if new_datetime > last_datetime:\n",
    "                # Verify and clean new data before appending\n",
    "                new_df = new_df[new_df['datetime'] > last_datetime]\n",
    "                \n",
    "                # Combine data\n",
    "                combined_df = pd.concat([main_df, new_df], ignore_index=True)\n",
    "                \n",
    "                # Save without index and with exact column order\n",
    "                combined_df.to_csv(\n",
    "                    'runningData15.csv',\n",
    "                    index=False,\n",
    "                    columns=['datetime', 'symbol', 'open', 'high', 'low', 'close', 'volume'],\n",
    "                    date_format='%Y-%m-%d %H:%M:%S'\n",
    "                )\n",
    "                print(f\"✅ Successfully appended {len(new_df)} new records\")\n",
    "                print(f\"Last timestamp now: {new_df['datetime'].iloc[-1]}\")\n",
    "                break\n",
    "                \n",
    "            elif new_datetime == last_datetime:\n",
    "                # print(\"🔄 Data already exists. No update needed.\")\n",
    "                time.sleep(60)\n",
    "                continue\n",
    "                \n",
    "            else:\n",
    "                # print(f\"⏳ New data ({new_datetime}) is older than last record ({last_datetime})\")\n",
    "                time.sleep(60)\n",
    "                continue\n",
    "                \n",
    "        except pd.errors.EmptyDataError:\n",
    "            print(\"⚠️ No data found in files. Retrying...\")\n",
    "            time.sleep(60)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error: {str(e)}\")\n",
    "            time.sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tvDatafeed.main:you are using nologin method, data you access may be limited\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              symbol      open     high       low    close  \\\n",
      "datetime                                                                     \n",
      "2025-03-28 02:45:00  FOREXCOM:XAUUSD  3056.475  3057.13  3056.155  3056.62   \n",
      "\n",
      "                     volume  \n",
      "datetime                     \n",
      "2025-03-28 02:45:00     0.0  \n",
      "Data saved to lastData.csv\n",
      "🔄 Data already exists. No update needed.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_new_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[38], line 44\u001b[0m, in \u001b[0;36mget_new_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m new_datetime \u001b[38;5;241m==\u001b[39m last_datetime:\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m🔄 Data already exists. No update needed.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 44\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m60\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "get_new_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIINTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune(model, X_test, y_test, scaler, y_test_original, results):\n",
    "    second_last = X_test[-2].reshape(1, sequence_length)\n",
    "    actual = y_test[-1].reshape(1, 1)\n",
    "    start_time = time.time()\n",
    "    model.fit(second_last,\n",
    "            actual,\n",
    "            epochs=20,\n",
    "            batch_size = 4048,\n",
    "            verbose=0)\n",
    "    model.save('fine_tune_model_march_27_2nd_trial.h5')\n",
    "    end_time = time.time()\n",
    "    print(f\"Model finetuned in {end_time - start_time} seconds\")\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(831, 1)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'scaler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m x_test, y_test, y_test_original \u001b[38;5;241m=\u001b[39m init()\n\u001b[1;32m      7\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 8\u001b[0m results \u001b[38;5;241m=\u001b[39m finetune(model, x_test, y_test, \u001b[43mscaler\u001b[49m, y_test_original, results)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'scaler' is not defined"
     ]
    }
   ],
   "source": [
    "model = load_model('fine_tune_model_march_27_2nd_trial.h5')\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.MeanSquaredError(),\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate * 0.1)  # Lower learning rate for fine-tuning\n",
    ")\n",
    "x_test, y_test, y_test_original = init()\n",
    "results = []\n",
    "results = finetune(model, x_test, y_test, scaler, y_test_original, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1849, 1)\n",
      "Predicted: [[3210.373]]\n",
      "Failed to upload prediction: 201 - {\"message\":\"Prediction created successfully\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model finetuned in 2.797517776489258 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tvDatafeed.main:you are using nologin method, data you access may be limited\n",
      "WARNING:tvDatafeed.main:you are using nologin method, data you access may be limited\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully appended 1 new records\n",
      "Last timestamp now: 2025-04-15 02:15:00\n",
      "(1850, 1)\n",
      "Predicted: [[3210.5854]]\n",
      "Failed to upload prediction: 201 - {\"message\":\"Prediction created successfully\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model finetuned in 0.6685924530029297 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tvDatafeed.main:you are using nologin method, data you access may be limited\n",
      "WARNING:tvDatafeed.main:you are using nologin method, data you access may be limited\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully appended 1 new records\n",
      "Last timestamp now: 2025-04-15 02:30:00\n",
      "(1851, 1)\n",
      "Predicted: [[3209.77]]\n",
      "Failed to upload prediction: 201 - {\"message\":\"Prediction created successfully\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model finetuned in 0.6712629795074463 seconds\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m current_minute \u001b[38;5;241m=\u001b[39m current_time\u001b[38;5;241m.\u001b[39mtm_min  \u001b[38;5;66;03m# Extract minute (0-59)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m remainder \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m15\u001b[39m \u001b[38;5;241m-\u001b[39m (current_minute \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m15\u001b[39m)\n\u001b[0;32m---> 17\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremainder\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m60\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m get_new_data()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = load_model('fine_tune_model_april_15_1st_trial.h5')\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.MeanSquaredError(),\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate * 0.1)  # Lower learning rate for fine-tuning\n",
    ")\n",
    "while(True):\n",
    "    \n",
    "    X_test_new, y_test_new, y_test_new_original = init()\n",
    "    results = []\n",
    "    results = assume_and_upload(model, X_test_new, y_test_new, scaler, y_test_new_original, results)\n",
    "    # assume_and_upload(model, X_test_new, y_test_new, scaler, y_test_new_original, results)\n",
    "    results = finetune(model, X_test_new, y_test_new, scaler, y_test_new_original, results)\n",
    "    current_timestamp = time.time()\n",
    "    current_time = time.localtime(current_timestamp)\n",
    "    current_minute = current_time.tm_min  # Extract minute (0-59)\n",
    "    remainder = 15 - (current_minute % 15)\n",
    "    time.sleep((remainder - 1) * 60)\n",
    "    get_new_data()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "model = load_model('fine_tune_model_march_27.h5')\n",
    "model.compile(\n",
    "        loss=tf.keras.losses.MeanSquaredError(),\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate * 0.1)  # Lower learning rate for fine-tuning\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: [[3024.6055]]\n",
      "Failed to upload prediction: 201 - {\"message\":\"Prediction created successfully\"}\n",
      "[array([[3024.6055]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "results = assume_and_upload(model, x, y, scaler, y_original, [])\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finbert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
